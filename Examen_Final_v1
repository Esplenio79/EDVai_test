##Ingestamos los archivos relacionados con transporte aéreo de Argentina con WGET desde:
2021:
https://dataengineerpublic.blob.core.windows.net/data-engineer/2021-informe-ministerio.csv
2022:
https://dataengineerpublic.blob.core.windows.net/data-engineer/202206-informe-ministerio.csv
Aeropuertos_detalles:
https://dataengineerpublic.blob.core.windows.net/data-engineer/aeropuertos_detalle.csv

##Los enviamos a hdfs y verificamos que se encuentran en la carpeta /ingest
plchiapero@LAPTOP-HJNIQGOK:~$ docker exec -it edvai_hadoop /bin/bash
root@7b946e8f971d:/# su hadoop
hadoop@7b946e8f971d:/$ hdfs dfs -ls /ingest/
Found 4 items
-rw-r--r--   1 hadoop supergroup   32322556 2024-09-29 17:21 /ingest/2021-informe-ministerio.csv
-rw-r--r--   1 hadoop supergroup   22833520 2024-09-29 18:09 /ingest/202206-informe-ministerio.csv
-rw-r--r--   1 hadoop supergroup     136007 2024-09-29 18:10 /ingest/aeropuertos_detalle.csv

hadoop@7b946e8f971d:/$ pyspark

##Cambiamos la separación de los datos de los archivos de ";" a ","
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/

Using Python version 3.8.10 (default, Mar 15 2022 12:22:08)
Spark context Web UI available at http://7b946e8f971d:4040
Spark context available as 'sc' (master = yarn, app id = application_1727916311286_0001).
SparkSession available as 'spark'.

>>> df2022 = spark.read.option("header", "true").option("sep", ";").csv("/ingest/202206-informe-ministerio.csv")
>>> df2021 = spark.read.option("header", "true").option("sep", ";").csv("/ingest/2021-informe-ministerio.csv")
>>> df2022.printSchema()
root
 |-- Fecha: string (nullable = true)
 |-- Hora UTC: string (nullable = true)
 |-- Clase de Vuelo (todos los vuelos): string (nullable = true)
 |-- Clasificación Vuelo: string (nullable = true)
 |-- Tipo de Movimiento: string (nullable = true)
 |-- Aeropuerto: string (nullable = true)
 |-- Origen / Destino: string (nullable = true)
 |-- Aerolinea Nombre: string (nullable = true)
 |-- Aeronave: string (nullable = true)
 |-- Pasajeros: string (nullable = true)
 |-- Calidad dato: string (nullable = true)

>>> df2021.printSchema()
root
 |-- Fecha: string (nullable = true)
 |-- Hora UTC: string (nullable = true)
 |-- Clase de Vuelo (todos los vuelos): string (nullable = true)
 |-- Clasificación Vuelo: string (nullable = true)
 |-- Tipo de Movimiento: string (nullable = true)
 |-- Aeropuerto: string (nullable = true)
 |-- Origen / Destino: string (nullable = true)
 |-- Aerolinea Nombre: string (nullable = true)
 |-- Aeronave: string (nullable = true)
 |-- Pasajeros: string (nullable = true)
 |-- Calidad dato: string (nullable = true)

##Unimos las dos tablas con UNION
>>> df_2122 = df2021.union(df2022)

##Creamos una vista de esa nueva tabla
>>> df_2122.createOrReplaceTempView("df_2122_v")

##Casteamos y verificamos los cambios con printSchema y show
>>> from pyspark.sql.functions import to_date, col
>>> df_2122_cast = df_2122.withColumn("fecha", to_date(col("fecha"), "dd/MM/yyyy")).withColumn("pasajeros", col("pasajeros").cast("int"))
>>> df_2122_cast.printSchema()
root
 |-- fecha: date (nullable = true)
 |-- Hora UTC: string (nullable = true)
 |-- Clase de Vuelo (todos los vuelos): string (nullable = true)
 |-- Clasificación Vuelo: string (nullable = true)
 |-- Tipo de Movimiento: string (nullable = true)
 |-- Aeropuerto: string (nullable = true)
 |-- Origen / Destino: string (nullable = true)
 |-- Aerolinea Nombre: string (nullable = true)
 |-- Aeronave: string (nullable = true)
 |-- pasajeros: integer (nullable = true)
 |-- Calidad dato: string (nullable = true)

>>> df_2122_cast.show(10)
+----------+--------+---------------------------------+-------------------+------------------+----------+----------------+--------------------+----------------+---------+------------+
|     fecha|Hora UTC|Clase de Vuelo (todos los vuelos)|Clasificación Vuelo|Tipo de Movimiento|Aeropuerto|Origen / Destino|    Aerolinea Nombre|        Aeronave|pasajeros|Calidad dato|
+----------+--------+---------------------------------+-------------------+------------------+----------+----------------+--------------------+----------------+---------+------------+
|2021-01-01|   00:02|             Vuelo Privado con...|          Domestico|          Despegue|       PAR|             ROS|                   0|    PA-PA-28-181|        0|  DEFINITIVO|
|2021-01-01|   00:24|                          Regular|          Domestico|        Aterrizaje|       EZE|             GRA|AEROLINEAS ARGENT...|     BO-B737-8MB|       70|  DEFINITIVO|
|2021-01-01|   00:26|                          Regular|          Domestico|        Aterrizaje|       EZE|             ECA|AEROLINEAS ARGENT...|      BO-737-800|       70|  DEFINITIVO|
|2021-01-01|   00:29|                          Regular|          Domestico|        Aterrizaje|       EZE|             SAL|AEROLINEAS ARGENT...|    BO-B-737-76N|       12|  DEFINITIVO|

##Generamos una vista de este nuevo df
>>> df_2122_cast.createOrReplaceTempView("df_2122_cast_v")

##Casteamos de acuerdo a lo solicitado en el punto del Schema 
>>> df_cast_nombres = spark.sql("SELECT to_date(fecha, 'dd/MM/yyyy') AS date, CAST(`Hora UTC` AS STRING) AS hora_utc, CAST(`Clase de Vuelo (todos los vuelos)` AS STRING) AS clase_de_vuelo, CAST(`Clasificación Vuelo` AS STRING) AS clasificacion_de_vuelo, CAST(`Tipo de Movimiento` AS STRING) AS tipo_de_movimiento, CAST(aeropuerto AS STRING) AS aeropuerto, CAST(`Origen / Destino` AS STRING) AS origen_destino, CAST(`Aerolinea Nombre` AS STRING) AS aerolinea_nombre, CAST(aeronave AS STRING) AS aeronave, COALESCE(CAST(pasajeros AS INTEGER), 0) AS pasajeros FROM df_2122_cast_v")
>>> df_cast_nombres.printSchema()
root
 |-- date: date (nullable = true)
 |-- hora_utc: string (nullable = true)
 |-- clase_de_vuelo: string (nullable = true)
 |-- clasificacion_de_vuelo: string (nullable = true)
 |-- tipo_de_movimiento: string (nullable = true)
 |-- aeropuerto: string (nullable = true)
 |-- origen_destino: string (nullable = true)
 |-- aerolinea_nombre: string (nullable = true)
 |-- aeronave: string (nullable = true)
 |-- pasajeros: integer (nullable = false)

>>> df_cast_nombres.show(2)
+----------+--------+--------------------+----------------------+------------------+----------+--------------+--------------------+------------+---------+
|      date|hora_utc|      clase_de_vuelo|clasificacion_de_vuelo|tipo_de_movimiento|aeropuerto|origen_destino|    aerolinea_nombre|    aeronave|pasajeros|
+----------+--------+--------------------+----------------------+------------------+----------+--------------+--------------------+------------+---------+
|2021-01-01|   00:02|Vuelo Privado con...|             Domestico|          Despegue|       PAR|           ROS|                   0|PA-PA-28-181|        0|
|2021-01-01|   00:24|             Regular|             Domestico|        Aterrizaje|       EZE|           GRA|AEROLINEAS ARGENT...| BO-B737-8MB|       70|
+----------+--------+--------------------+----------------------+------------------+----------+--------------+--------------------+------------+---------+
only showing top 2 rows

>>> df_cast_nombres.createOrReplaceTempView("df_2122_cast_v_final")
>>> spark.sql("DESCRIBE df_2122_cast_v_final").show()
+--------------------+---------+-------+
|            col_name|data_type|comment|
+--------------------+---------+-------+
|                date|     date|   null|
|            hora_utc|   string|   null|
|      clase_de_vuelo|   string|   null|
|clasificacion_de_...|   string|   null|
|  tipo_de_movimiento|   string|   null|
|          aeropuerto|   string|   null|
|      origen_destino|   string|   null|
|    aerolinea_nombre|   string|   null|
|            aeronave|   string|   null|
|           pasajeros|      int|   null|
+--------------------+---------+-------+

## En HIVE, damos de alta la BD que llamamos examen_final, luego chequeamos el alta correcta, y finalmente elegimos usar la BD que dimos de alta

hive> create database examen_final;
hive> show databases;
OK
examen_final
Time taken: 0.027 seconds, Fetched: 1 row(s)
hive> use examen_final;
OK

##Damos de alta la tabla 
hive> CREATE EXTERNAL TABLE IF NOT EXISTS examen_final.aeropuerto_tabla (fecha date, hora_utc string, clase_de_vuelo string, clasificacion_de_vuelo string, tipo_de_movimiento string, aeropuerto string, origen_destino string, aerolinea_nombre string, aeronave string, pasajeros int)
    > row format delimited
    > fields terminated by ','
    > location '/tables/external/examen_final';
OK
Time taken: 0.863 seconds
hive>

##Vuelvo a spark, y envío la vista del df a hive

>>> spark.sql("insert into examen_final.aeropuerto_tabla select * from df_2122_cast_v_final")
2024-11-24 19:59:11,787 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
DataFrame[]
>>>

#Chequeo que en hive esté la tabla

XXXXXXX

df_cast_nombres = spark.sql("""SELECT CAST(fecha AS DATE) AS fecha, CAST(Hora_UTC AS STRING) AS hora_UTC, CAST("Clase de Vuelo (todos los vuelos)" AS STRING) AS clase_de_vuelo,CAST(Clasificación_Vuelo AS STRING) AS clasificacion_de_vuelo, CAST(Tipo_de_Movimiento AS STRING) AS tipo_de_movimiento, CAST(Aeropuerto AS STRING) AS aeropuerto, CAST("Origen / Destino" AS STRING) AS origen_destino, CAST(Aerolinea_Nombre AS STRING) AS aerolinea_nombre, CAST(Aeronave AS STRING) AS aeronave, CAST(pasajeros AS INT) AS pasajeros FROM df_2122_cast_v""")


